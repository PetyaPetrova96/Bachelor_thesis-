# ğŸ‘¶ Affect Detection in Infants using Vision Transformers (ViT)

This project fine-tunes a Vision Transformer (ViT) to classify infant facial expressions into **Positive**, **Neutral**, and **Negative**, using the *City Infant Faces* dataset. It also visualizes model decisions using **SmoothGrad**.

---

## ğŸ“‚ Project Structure
â”œâ”€â”€ EDA.py # Exploratory Data Analysis <br>
â”œâ”€â”€ data_processing.py # Preprocessing and DataLoaders <br>
â”œâ”€â”€ model_training.py # ViT fine-tuning <br>
â”œâ”€â”€ model_testing.py # Evaluation & misclassification analysis <br>
â”œâ”€â”€ smoothgrad.py # Visualizing with SmoothGrad <br>
â”œâ”€â”€ trained_vit_model.pth # Saved model weights <br>


## ğŸ“¦ Requirements

- Python 3.9+
- PyTorch
- Hugging Face `transformers`, `datasets`
- OpenCV
- Matplotlib
- tqdm

  ## ğŸ§  Model Overview

- **Base:** ViT-Base-Patch16-224-in21k  
- **Pretrained on:** AffectNet, FER2013, MMI  
- **Fine-tuned on:** City Infant Faces (Color images)  
- **Classification Labels:** Positive, Neutral, Negative  

**Accuracy Results:**

| Dataset  | Accuracy |
|----------|----------|
| Dev Set  | 92%      |
| Test Set | 84%      |

---

## ğŸ” SmoothGrad Insights

**Highlighted facial regions by class:**

- **Positive:** Cheeks & lip corners *(AU6, AU12)*
- **Neutral:** T-zone (forehead, nose, mouth)
- **Negative:** Lip stretch, chin, eye tension *(AU20, AU17, AU6/7)*
